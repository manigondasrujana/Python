{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"santander.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 371)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable will be \"TARGET\"\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(labels = ['TARGET'], axis = 1), \n",
    "                                                    data['TARGET'], \n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  202\n"
     ]
    }
   ],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(x_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53214, 168), (22806, 168))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "x_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "x_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important\n",
    "\n",
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear models benefit from feature scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here I will do the model fitting and feature selection\n",
    "# altogether in one line of code\n",
    "\n",
    "# first I specify the Logistic Regression model, and I\n",
    "# make sure I select the Lasso (l1) penalty.\n",
    "\n",
    "# Then I use the selectFromModel object from sklearn, which\n",
    "# will select in theory the features which coefficients are non-zero\n",
    "\n",
    "sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1'))\n",
    "sel_.fit(scaler.transform(x_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False,  True,  True,  True,\n",
       "       False,  True,  True,  True, False,  True,  True,  True, False,\n",
       "        True,  True,  True, False, False, False, False,  True, False,\n",
       "        True,  True,  True,  True,  True, False,  True, False, False,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "       False, False, False, False,  True, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True, False, False,  True, False,\n",
       "       False, False, False,  True, False,  True, False,  True,  True,\n",
       "        True, False,  True,  True,  True, False, False, False, False,\n",
       "       False,  True, False, False, False, False,  True,  True, False,\n",
       "       False, False,  True,  True, False,  True, False, False,  True,\n",
       "       False, False,  True,  True, False, False, False, False, False,\n",
       "       False,  True,  True,  True, False, False, False,  True,  True,\n",
       "       False, False, False,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False, False,  True, False, False,  True,\n",
       "        True,  True, False, False, False, False, False,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True, False, False,\n",
       "       False, False, False,  True, False,  True])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command let's me visualise those features that were kept\n",
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 168\n",
      "selected features: 91\n",
      "features with coefficients shrank to zero: 77\n"
     ]
    }
   ],
   "source": [
    "# Now I make a list with the selected features\n",
    "selected_feat = x_train.columns[(sel_.get_support())]\n",
    "\n",
    "print('total features: {}'.format((x_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ind_var2_0', 'ind_var2', 'ind_var6', 'ind_var13_medio_0',\n",
       "       'ind_var18_0', 'ind_var27_0', 'ind_var28_0', 'ind_var28', 'ind_var27',\n",
       "       'ind_var31_0', 'ind_var41', 'ind_var46_0', 'ind_var46',\n",
       "       'num_op_var40_hace3', 'num_var27_0', 'num_var28_0', 'num_var28',\n",
       "       'num_var27', 'num_var41', 'num_var46_0', 'num_var46', 'saldo_var17',\n",
       "       'saldo_var20', 'saldo_var28', 'saldo_var27', 'saldo_var30',\n",
       "       'saldo_var33', 'saldo_var41', 'saldo_var46',\n",
       "       'delta_imp_aport_var33_1y3', 'delta_imp_reemb_var33_1y3',\n",
       "       'delta_imp_trasp_var17_in_1y3', 'delta_imp_trasp_var17_out_1y3',\n",
       "       'delta_imp_trasp_var33_in_1y3', 'delta_imp_trasp_var33_out_1y3',\n",
       "       'delta_num_reemb_var33_1y3', 'delta_num_trasp_var33_out_1y3',\n",
       "       'imp_amort_var18_hace3', 'imp_amort_var34_hace3',\n",
       "       'imp_aport_var17_ult1', 'imp_aport_var33_hace3', 'imp_aport_var33_ult1',\n",
       "       'imp_compra_var44_hace3', 'imp_reemb_var13_hace3',\n",
       "       'imp_reemb_var17_hace3', 'imp_reemb_var33_hace3',\n",
       "       'imp_reemb_var33_ult1', 'imp_trasp_var17_in_hace3',\n",
       "       'imp_trasp_var17_out_hace3', 'imp_trasp_var33_in_hace3',\n",
       "       'imp_trasp_var33_out_hace3', 'imp_trasp_var33_out_ult1',\n",
       "       'imp_venta_var44_hace3', 'var21', 'num_var2_0_ult1', 'num_var2_ult1',\n",
       "       'num_aport_var17_ult1', 'num_aport_var33_hace3',\n",
       "       'num_compra_var44_hace3', 'num_var22_hace3', 'num_reemb_var13_hace3',\n",
       "       'num_reemb_var17_hace3', 'num_reemb_var33_hace3',\n",
       "       'num_reemb_var33_ult1', 'num_trasp_var11_ult1',\n",
       "       'num_trasp_var17_out_hace3', 'num_trasp_var33_out_hace3',\n",
       "       'num_trasp_var33_out_ult1', 'num_venta_var44_ult1', 'saldo_var2_ult1',\n",
       "       'saldo_medio_var5_hace3', 'saldo_medio_var13_largo_hace3',\n",
       "       'saldo_medio_var13_medio_hace3', 'saldo_medio_var29_hace2',\n",
       "       'saldo_medio_var29_hace3', 'saldo_medio_var33_hace3',\n",
       "       'saldo_medio_var44_hace3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can identify the removed features like this:\n",
    "# numpy.ravel(array, order = ‘C’) : returns contiguous flattened array\n",
    "# (1D array with all the input-array elements and with the same type as it). \n",
    "removed_feats = x_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53214, 91), (22806, 91))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can then remove the features from the training and testing set\n",
    "# like this\n",
    "x_train_selected = sel_.transform(x_train.fillna(0))\n",
    "x_test_selected = sel_.transform(x_test.fillna(0))\n",
    "\n",
    "x_train_selected.shape, x_test_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features by random forests derived importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features using the impotance derived from\n",
    "# random forests\n",
    "\n",
    "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=400))\n",
    "sel_.fit(x_train, y_train)\n",
    "\n",
    "# remove features with zero coefficient from dataset\n",
    "# and parse again as dataframe (output of sklearn is\n",
    "# numpy array)\n",
    "x_train_rf = pd.DataFrame(sel_.transform(x_train))\n",
    "x_test_rf = pd.DataFrame(sel_.transform(x_test))\n",
    "\n",
    "# add the columns name\n",
    "x_train_rf.columns = x_train.columns[(sel_.get_support())]\n",
    "x_test_rf.columns = x_train.columns[(sel_.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rf.shape, X_test_rf.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
